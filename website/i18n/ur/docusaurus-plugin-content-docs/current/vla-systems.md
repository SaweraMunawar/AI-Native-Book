---
sidebar_position: 5
title: Vision-Language-Action سسٹمز
description: VLA ماڈلز کے بارے میں جانیں جو ادراک، زبان، اور روبوٹ کنٹرول کو یکجا کرتے ہیں
---

# Vision-Language-Action سسٹمز

Vision-Language-Action (VLA) ماڈلز روبوٹ لرننگ کی جدید ترین سرحد کی نمائندگی کرتے ہیں، بصری ادراک، زبان کی سمجھ، اور عمل کی پیداوار کو متحد نظاموں میں یکجا کرتے ہیں۔

## VLA انقلاب

روایتی روبوٹ پروگرامنگ میں واضح سٹیٹ مشینز اور ہینڈ کوڈڈ رویوں کی ضرورت ہوتی تھی۔ VLA ماڈلز اسے تبدیل کرتے ہیں:

1. **قدرتی زبان کی ہدایات سمجھنا**
2. **بصری منظر کو سمجھنا**
3. **مناسب روبوٹ ایکشنز پیدا کرنا**

```
┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│   بصری     │ → │   زبان      │ → │   عمل      │
│   ان پٹ    │    │  ہدایت      │    │  آؤٹ پٹ    │
│  (کیمرہ)   │    │  ("کپ      │    │  (موٹر     │
│             │    │   اٹھاؤ")   │    │  کمانڈز)  │
└─────────────┘    └─────────────┘    └─────────────┘
```

## روبوٹکس کے لیے فاؤنڈیشن ماڈلز

### بڑے لینگویج ماڈلز (LLMs)

LLMs استدلال اور منصوبہ بندی کی صلاحیتیں فراہم کرتے ہیں:

- **GPT-4** - ٹاسک ڈیکمپوزیشن اور پلاننگ
- **PaLM** - چین آف تھاٹ ریزننگ
- **LLaMA** - اوپن سورس فاؤنڈیشن

روبوٹکس میں LLM کے کردار:
- اعلیٰ سطحی ٹاسک پلاننگ
- ایرر ریکوری ریزننگ
- ہیومن روبوٹ ڈائیلاگ
- روبوٹ رویوں کے لیے کوڈ جنریشن

### Vision-Language ماڈلز (VLMs)

VLMs بصری اور متنی سمجھ کو یکجا کرتے ہیں:

| ماڈل | صلاحیتیں |
|-------|-------------|
| CLIP | امیج-ٹیکسٹ میچنگ |
| BLIP-2 | ویژول کوئسچن آنسرنگ |
| GPT-4V | ویژول ریزننگ |
| LLaVA | اوپن سورس ملٹی موڈل |

### VLA ماڈل کی مثالیں

جدید ترین VLA ماڈلز:

1. **RT-2** (Google DeepMind)
   - روبوٹ ڈیمونسٹریشنز + ویب ڈیٹا پر ٹرین کیا گیا
   - براہ راست روبوٹ ایکشنز آؤٹ پٹ کرتا ہے
   - نئی اشیاء اور ہدایات پر جنرلائز کرتا ہے

2. **OpenVLA**
   - اوپن سورس VLA ماڈل
   - Llama 2 بیک بون پر مبنی
   - Open X-Embodiment ڈیٹاسیٹ پر ٹرین کیا گیا

3. **Octo**
   - ٹرانسفارمر بیسڈ جنرلسٹ پالیسی
   - ملٹی ٹاسک، ملٹی روبوٹ
   - نئے ٹاسکس پر فائن ٹیون ایبل

## VLA آرکیٹیکچر

### عام VLA پائپ لائن

```
     ┌──────────────────────────────────────────────┐
     │            VLA ماڈل آرکیٹیکچر               │
     └──────────────────────────────────────────────┘

┌─────────┐   ┌─────────────┐   ┌─────────────────┐
│  امیج   │ → │   ویژن     │ → │                 │
│ انکوڈر  │   │  ایمبیڈنگ  │   │                 │
└─────────┘   └─────────────┘   │                 │
                                │   ٹرانسفارمر   │
┌─────────┐   ┌─────────────┐   │    ڈیکوڈر      │
│لینگویج │ → │   ٹیکسٹ    │ → │                 │
│ٹوکنائزر│   │  ایمبیڈنگ  │   │                 │
└─────────┘   └─────────────┘   │                 │
                                └────────┬────────┘
                                         │
                                         ▼
                                ┌─────────────────┐
                                │  ایکشن ٹوکنز   │
                                │  (ڈسکریٹائزڈ)  │
                                └─────────────────┘
```

### ایکشن ٹوکنائزیشن

روبوٹ ایکشنز ٹوکنز میں تبدیل ہوتے ہیں:

```python
# مسلسل ایکشن سپیس
action = [x, y, z, roll, pitch, yaw, gripper]

# ہر ڈائمینشن کے لیے 256 بنز میں ڈسکریٹائز
action_tokens = [128, 64, 200, 128, 130, 125, 255]

# ووکیبلری: action_token_id = dimension * 256 + bin
```

### ٹریننگ ڈیٹا

VLA ماڈلز کو متنوع ٹریننگ ڈیٹا کی ضرورت ہے:

| ڈیٹاسیٹ | سائز | روبوٹ | ٹاسکس |
|---------|------|-------|-------|
| RT-1 | 130K ایپیسوڈز | Everyday Robots | 700+ ٹاسکس |
| Bridge | 50K ایپیسوڈز | WidowX | مینیپولیشن |
| Open X-Embodiment | 1M+ ایپیسوڈز | 22 روبوٹس | 500+ ٹاسکس |

## بہیویئر کلوننگ

VLA ماڈلز کے لیے بنیادی ٹریننگ اپروچ۔

### امیٹیشن لرننگ

ماہر ڈیمونسٹریشنز سے سیکھیں:

```python
# بہیویئر کلوننگ لاس
def bc_loss(model, observations, actions):
    predicted_actions = model(observations)
    loss = mse_loss(predicted_actions, actions)
    return loss

# ٹریننگ لوپ
for batch in dataloader:
    obs, actions = batch
    loss = bc_loss(model, obs, actions)
    loss.backward()
    optimizer.step()
```

### چیلنجز

بہیویئر کلوننگ کی حدود:
- **ڈسٹریبیوشن شفٹ** - ماڈل وہ سٹیٹس دیکھتا ہے جن پر ٹرین نہیں ہوا
- **کمپاؤنڈنگ ایررز** - چھوٹی غلطیاں جمع ہوتی ہیں
- **ملٹی موڈل ایکشنز** - متعدد درست ایکشنز موجود ہیں

حل:
- ڈیٹا آگمینٹیشن
- DAgger (انٹرایکٹو کریکشن)
- ڈفیوژن پالیسیز

## ڈفیوژن پالیسیز

ڈفیوژن ماڈلز روبوٹ ایکشنز جنریٹ کرتے ہیں۔

### ڈفیوژن کیسے کام کرتا ہے

1. رینڈم نوائز سے شروع کریں
2. ایکشن ڈسٹریبیوشن کی طرف بتدریج ڈینوائز کریں
3. ریفائنڈ ایکشن سیکوینس آؤٹ پٹ کریں

```python
# آسان ڈفیوژن پالیسی
def diffusion_policy(observation, num_steps=50):
    # نوائز سے شروع کریں
    action = torch.randn(action_dim)

    for t in reversed(range(num_steps)):
        # نوائز پریڈکٹ کریں
        noise_pred = model(observation, action, t)
        # نوائز ہٹائیں
        action = denoise_step(action, noise_pred, t)

    return action
```

### فوائد

ڈفیوژن پالیسیز ان میں بہترین ہیں:
- ملٹی موڈل ایکشن ڈسٹریبیوشنز
- لانگ ہورائزن پلاننگ
- کانٹیکٹ رچ مینیپولیشن

## لینگویج گراؤنڈنگ

زبان کو روبوٹ ایکشنز سے جوڑنا۔

### ہدایات کی پیروی

```
ہدایت: "سرخ کپ اٹھاؤ اور میز پر رکھو"

گراؤنڈنگ:
- "اٹھاؤ" → گرپ ایکشن
- "سرخ کپ" → آبجیکٹ ڈیٹیکشن + سلیکشن
- "رکھو" → ریلیز ایکشن
- "میز پر" → ٹارگٹ لوکیشن
```

### مقامی استدلال

VLA ماڈلز مقامی تصورات سیکھتے ہیں:
- بائیں/دائیں، اوپر/نیچے
- قریب/دور، اندر/باہر
- اشیاء کے درمیان نسبتی پوزیشنز

### آبجیکٹ افورڈینسز

سمجھنا کہ اشیاء کون سے ایکشنز سپورٹ کرتی ہیں:
- کپ پکڑے، بھرے، رکھے جا سکتے ہیں
- دروازے کھولے، بند، دھکیلے جا سکتے ہیں
- دراز کھینچے، دھکیلے جا سکتے ہیں

## جانچ

### بینچ مارکس

عام VLA جانچ بینچ مارکس:

| بینچ مارک | فوکس | میٹرکس |
|-----------|-------|---------|
| CALVIN | زبان-کنڈیشنڈ | کامیابی کی شرح |
| SIMPLER | سمیولیشن ٹرانسفر | ٹاسک مکمل ہونا |
| RLBench | مینیپولیشن | 100 ڈیموز پر کامیابی |
| Open X-Eval | جنرلائزیشن | کراس روبوٹ ٹرانسفر |

### میٹرکس

اہم کارکردگی میٹرکس:
- **کامیابی کی شرح** - ٹاسک مکمل ہونے کا فیصد
- **جنرلائزیشن** - ان دیکھی اشیاء پر کارکردگی
- **سیمپل ایفیشنسی** - سیکھنے کے لیے ڈیموز کی ضرورت
- **ایگزیکیوشن ٹائم** - ٹاسک مکمل ہونے کی رفتار

## VLA ماڈلز کی تعیناتی

### ہارڈ ویئر کی ضروریات

VLA انفرنس کی ضرورت:
- ویژن انکوڈر کے لیے GPU (RTX 3090+)
- کنٹرول لوپ کے لیے تیز CPU
- کم تاخیر والی کیمرہ پائپ لائن

### ریئل ٹائم غور و خوض

```python
# کنٹرول لوپ ٹائمنگ
CONTROL_FREQUENCY = 10  # Hz
INFERENCE_BUDGET = 80   # ms

def control_loop():
    while running:
        start = time.time()

        # آبزرویشن حاصل کریں
        image = camera.capture()

        # VLA انفرنس
        action = vla_model(image, instruction)

        # ایکشن ایگزیکیوٹ کریں
        robot.execute(action)

        # فریکوینسی برقرار رکھیں
        elapsed = time.time() - start
        sleep(1/CONTROL_FREQUENCY - elapsed)
```

## خلاصہ

اہم VLA تصورات:
- VLA ماڈلز ویژن، زبان، اور عمل کو متحد کرتے ہیں
- فاؤنڈیشن ماڈلز پری ٹرین صلاحیتیں فراہم کرتے ہیں
- بہیویئر کلوننگ ڈیمونسٹریشنز سے ٹرین کرتی ہے
- ڈفیوژن پالیسیز ملٹی موڈل ایکشنز ہینڈل کرتی ہیں
- لینگویج گراؤنڈنگ ہدایات کو ایکشنز سے جوڑتی ہے
- ریئل ٹائم تعیناتی محتاط انجینئرنگ کی متقاضی ہے

---

← **پچھلا:** [ڈیجیٹل ٹوئن سمیولیشن](./digital-twin.md) | **اگلا:** [کیپسٹون پروجیکٹ](./capstone.md) →
