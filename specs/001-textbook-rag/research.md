# Research: AI-Native Textbook with RAG Chatbot

**Feature**: 001-textbook-rag
**Date**: 2025-12-09
**Status**: Complete

## Research Tasks

### RT-001: Docusaurus Auto-Sidebar Configuration

**Question**: How to configure Docusaurus for automatic sidebar generation from markdown files?

**Decision**: Use `autogenerated` sidebar configuration with `docs` directory structure

**Rationale**:
- Docusaurus 3.x supports `sidebars.js` with `type: 'autogenerated'`
- Files are ordered by filename prefix (e.g., `01-intro.md`, `02-basics.md`)
- Markdown frontmatter `sidebar_position` provides fine-grained control
- No manual sidebar maintenance required

**Configuration**:
```javascript
// sidebars.js
module.exports = {
  tutorialSidebar: [
    {
      type: 'autogenerated',
      dirName: '.',
    },
  ],
};
```

**Alternatives Considered**:
- Manual sidebar definition: Rejected (maintenance overhead, violates Minimalism)
- Custom plugin: Rejected (unnecessary complexity)

---

### RT-002: Qdrant Cloud Free Tier Limits

**Question**: What are Qdrant Cloud free tier limits and are they sufficient?

**Decision**: Qdrant Cloud free tier is sufficient for this project

**Rationale**:
- Free tier: 1GB storage, 1 cluster
- Estimated content: 50-100 pages = ~200-400 chunks @ 512 tokens each
- Estimated storage: ~200KB-1MB with 384-dim embeddings
- Well within free tier limits

**Limits Verified**:
| Resource | Free Tier | Our Need | Margin |
|----------|-----------|----------|--------|
| Storage | 1GB | ~1MB | 1000x |
| Collections | Unlimited | 1 | ✓ |
| Vectors | Unlimited | ~400 | ✓ |

**Alternatives Considered**:
- Pinecone: Similar free tier, but Qdrant has better Python SDK
- Chroma (local): Rejected (requires persistent server, complicates deployment)

---

### RT-003: Groq API Free Tier and Rate Limits

**Question**: What are Groq's free tier limits for Llama 3?

**Decision**: Groq free tier is sufficient for 100 DAU

**Rationale**:
- Groq free tier: 14,400 requests/day for Llama 3 8B
- Estimated usage: 100 DAU × 10 queries/user = 1,000 requests/day
- 14x headroom on rate limits
- Response latency: ~500ms-1s (fits <5s target)

**Limits Verified**:
| Resource | Free Tier | Our Need | Margin |
|----------|-----------|----------|--------|
| Requests/day | 14,400 | ~1,000 | 14x |
| Tokens/minute | 30,000 | ~5,000 | 6x |
| Context window | 8,192 | ~2,000 | 4x |

**Alternatives Considered**:
- OpenAI GPT-3.5: Pay-per-use, no free tier for sustained usage
- HuggingFace Inference: Higher latency, less consistent
- Ollama (local): Requires always-on server

---

### RT-004: HuggingFace Embeddings Free Tier

**Question**: Can we use HuggingFace Inference API for free embeddings?

**Decision**: Use `sentence-transformers/all-MiniLM-L6-v2` via HuggingFace Inference API

**Rationale**:
- Free tier: 1,000 requests/day (sufficient for ingestion + queries)
- Model produces 384-dim vectors (compact, fast similarity search)
- Quality sufficient for educational Q&A (MRR ~0.8 on similar tasks)

**Alternative Approach**:
- Run embeddings locally during ingestion (one-time)
- Use cached embeddings at query time, or embed locally in backend
- This hybrid avoids API rate limits entirely

**Final Decision**: Hybrid approach
1. Ingestion: Run locally with sentence-transformers library
2. Query: Run locally in backend (no API call needed)

**Alternatives Considered**:
- OpenAI ada-002: Cost per token
- Cohere: Free tier too limited
- Larger models: Slower, more storage

---

### RT-005: Backend Deployment Options

**Question**: Which free-tier platform for FastAPI backend?

**Decision**: Railway free tier (primary), Vercel Serverless (backup)

**Rationale**:
- Railway: $5 free credit/month, persistent process, simple deploy
- Vercel: True serverless, cold starts, but GitHub Pages + Vercel is common pattern
- Both support Python/FastAPI

**Comparison**:
| Feature | Railway | Vercel Serverless |
|---------|---------|-------------------|
| Cold start | None | ~1-3s |
| Free tier | $5/month credit | 100GB-hrs |
| Python support | Native | Via serverless |
| Deployment | Git push | Git push |

**Decision**: Start with Railway for simpler development, migrate to Vercel if needed for scale.

**Alternatives Considered**:
- Render: Similar to Railway, longer cold starts
- Fly.io: More complex setup
- Self-hosted: Violates free-tier principle

---

### RT-006: Select-to-Ask UI Implementation

**Question**: How to implement text selection → Ask AI in Docusaurus?

**Decision**: Custom React component using `window.getSelection()` API

**Rationale**:
- Docusaurus is React-based, supports custom components
- `window.getSelection()` provides selected text
- Show floating "Ask AI" button on selection via mouseup event
- Click opens chatbot drawer with pre-filled context

**Implementation Approach**:
```jsx
// SelectToAsk component
useEffect(() => {
  const handleMouseUp = () => {
    const selection = window.getSelection().toString().trim();
    if (selection.length > 10) { // Min selection threshold
      setSelectedText(selection);
      setShowButton(true);
    }
  };
  document.addEventListener('mouseup', handleMouseUp);
  return () => document.removeEventListener('mouseup', handleMouseUp);
}, []);
```

**Alternatives Considered**:
- Browser extension: Out of scope, complex distribution
- Highlight.js plugin: Doesn't support this use case

---

### RT-007: Confidence Threshold Implementation

**Question**: How to implement retrieval confidence scoring and disclaimers?

**Decision**: Use Qdrant similarity score with threshold of 0.7

**Rationale**:
- Qdrant returns cosine similarity scores (0-1 range)
- Score < 0.7: Add "Based on limited context..." disclaimer
- Score < 0.4: Return "This topic may not be covered in the textbook"
- Scores tested empirically on sample queries

**Implementation**:
```python
def get_response_with_confidence(query: str) -> ChatResponse:
    results = qdrant.search(query, limit=3)
    top_score = results[0].score if results else 0

    if top_score < 0.4:
        return ChatResponse(
            answer="This topic may not be covered in this textbook.",
            confidence="low",
            sources=[]
        )
    elif top_score < 0.7:
        # Generate with disclaimer
        answer = generate_answer(query, results)
        return ChatResponse(
            answer=f"Based on limited context: {answer}",
            confidence="medium",
            sources=[r.payload for r in results]
        )
    else:
        # High confidence
        answer = generate_answer(query, results)
        return ChatResponse(
            answer=answer,
            confidence="high",
            sources=[r.payload for r in results]
        )
```

**Alternatives Considered**:
- Binary threshold: Too rigid
- LLM self-assessment: Adds latency and cost

---

### RT-008: Neon PostgreSQL Usage

**Question**: What should Neon store vs Qdrant?

**Decision**: Use Neon for chat session metadata only, not embeddings

**Rationale**:
- Qdrant handles all vector operations (embeddings, search)
- Neon stores: chat sessions, message history (optional), rate limit counters
- Keeps architecture simple (vectors in vector DB, relational data in SQL)
- Free tier: 0.5GB storage, sufficient for metadata

**Schema** (minimal):
```sql
CREATE TABLE chat_sessions (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  created_at TIMESTAMP DEFAULT NOW(),
  message_count INT DEFAULT 0
);

CREATE TABLE rate_limits (
  ip_hash VARCHAR(64) PRIMARY KEY,
  request_count INT DEFAULT 0,
  window_start TIMESTAMP DEFAULT NOW()
);
```

**Alternatives Considered**:
- Skip Neon entirely: Lose rate limiting capability
- Store embeddings in Neon with pgvector: Adds complexity, Qdrant is purpose-built

---

## Summary

All research tasks complete. No NEEDS CLARIFICATION items remain.

| Task | Decision | Risk Level |
|------|----------|------------|
| RT-001 | Docusaurus autogenerated sidebar | Low |
| RT-002 | Qdrant Cloud free tier | Low |
| RT-003 | Groq Llama 3 free tier | Medium (rate limits) |
| RT-004 | Local sentence-transformers | Low |
| RT-005 | Railway deployment | Low |
| RT-006 | Custom React selection component | Low |
| RT-007 | 0.7 confidence threshold | Medium (tune later) |
| RT-008 | Neon for metadata only | Low |

**Next Step**: Proceed to Phase 1 (Data Model & Contracts)
